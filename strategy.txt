Пока описываю только идею в структуре как могу. Дальше будем структурировать ее в понятный пайплайн для визуализации идеи и презентации команде. После на базе этого артефакта сгенерируем артефакт для инвесторов.

В чем наша текущая проблема: наша идея базируется на том, что мы сможем придумать такое покрытие больших моделей кастомными юз кейсами6 что мы сможем взять generalist модель open ai или anthropic и на базе нее построить решение.

Проблема в том, что покрытие интерфейсами и юзер флоу на сегодняшний день - это легко копируемый артефакт, который с вабкодингом облегчился кратно. Это значит, что мы находимся на алом океане где: множество новых игроков с или без доменной экспертизы попробуют взять те же всем доступные коммон решения и будут биться за звание лучшего.

Компани ис многолетней экспертизой тоже вовклекутся в эту гонку и будут выигрывать свою часть рынка за счет глубокой экспертизы вендора, которая у них накоплена и им не требуется ее приобретать, что требует и ресурсов и времени. Это из сильная сторона.

Чтобы понять как не ввязываться в гонку с множеством игроков на общих с ними решениями надо посмотреть на то, что делает компания apple.

Apple единственная коммпания которая не делает свой ИИ. В последнем апдейте они просто интегрировали в сири gemini что казалось бы совершенно не логично. Однако Apple на 2-3 шага опережает всех конкурентов в решениях для локального инференса (см. arxiv 2511.05502: Production-Grade Local LLM Inference on Apple Silicon).

При этом за время, что другие компании играли во фронтир, локальные вполне дошли до приемлемого уровня. гпт осс, квены, glm и всякое вот это, действительно неплохо на макбуках работающее. 20B модель работает на любом MacBook с чипом Apple Silicon (M1 и новее) с 16GB+ RAM — а это все Mac'и выпущенные с конца 2020 года. К 2026 году это уже 5+ лет устройств на рынке, и такие машины стали стандартом в tech-компаниях. Качество инференса на M-чипах сопоставимо с GPT-4.

=== КАКИЕ МОДЕЛИ РАБОТАЮТ НА M4/M5 (2025-2026) ===

Источники: Apple MLX Research, 9to5Mac, AppleMagazine

Hardware capabilities:
- M4 (120 GB/s bandwidth): 8B models в BF16, 14B-30B в 4-bit quantized
- M5 (153 GB/s bandwidth, +28%): 19-27% быстрее M4, time-to-first-token <10 сек для 14B dense, <3 сек для 30B MoE
- MacBook Pro 24GB: комфортно держит 8B BF16 или 30B MoE 4-bit (<18GB RAM usage)

Модели, протестированные Apple на M5:
- Qwen 1.7B, 8B (BF16)
- Qwen 8B, 14B (4-bit quantized)
- Qwen 30B MoE (3B active params, 4-bit)
- GPT-OSS 20B (MXFP4)

Популярные модели для локального запуска (2025):
- Llama 4 (8B, 70B) — самая универсальная open-weight семья
- Mistral Small 3 (24B) — state-of-the-art на бенчмарках, длинный контекст
- Qwen 2.5 серия — отличное соотношение качество/размер
- Gemma 3 — хорошо fine-tuned для специфичных задач
- Phi-4 — компактная модель от Microsoft

Скорость на M3/M4: 60-120 tokens/sec для 7-8B моделей (llama.cpp, Ollama, LM Studio).

Идея "эппл проиграл ИИ гонку" изначально выходит из посыла, что она в той же гонке, что и все остальные, что принципиально не так.

Чатгпт - венчур который продастся майкрософту. Антропик - амазону. Гугл - конкурирует с ними в части кода и поиска, aka ежедневного использования. Китайцы и европейцы играют в суверенитет. Все остальные тут вообще зачем? Эппл тут конкретно зачем?


Предыдущие 3 абзаца - это тезисы одного аналитика и они выглядят очень интересно. Выходит, что модели уровня GPT 4 уже сейчас могут быть развернуты локально на компьютере пользователя. А мы только в начале. Это значит, что generic модели будут на каждом компьютере в будущем на горизонте 5 лет.

И качество этих моделей будет достаточно для большинства повседневных задач.

Есть и другое наблюдение (подтверждено исследованиями): меньшие модели которые прошли fine-tuning на конкретную тему лучше работают чем большие generic модели. Примеры:
- Together AI: Gemma 3 27B fine-tuned превзошла Claude Sonnet 4 на 60% в специализированных задачах
- NVIDIA: Llama 3 8B с LoRA превзошла Llama 3 70B и Nemotron 340B в code review
- arxiv 2406.08660: fine-tuned BERT значительно лучше ChatGPT/Claude в text classification

LoRA адаптеры расширяющие и тюнящие модель на конкретную тему весят 10-20 МБ (vs гигабайты для full model).

Это все наводит на мысль, что выиграть рынок можно, если суже сейчас начать работать с малыми моделями и занятся их тюнингом под задачи комплаенса.

=== ПОЧЕМУ COMPLIANCE И АРХИТЕКТУРА ПРОДУКТА ===

Почему именно compliance:
- Это наша доменная экспертиза
- Compliance — не центральный фокус для OpenAI/Anthropic, слишком нишево для них
- Они делают большие дорогие generic модели, мы — маленькие специализированные

Архитектура: ЯДРО + ПЛАГИНЫ
- Ядро: compliance generic small model (базовая модель для compliance задач)
- Плагины: специализированные модули по 10-20 МБ под конкретные задачи (AML, KYC, sanctions, tax и т.д.)
- Каждый плагин в своём классе рвёт любую generic модель

Это наш moat:
- OpenAI/Anthropic могут скопировать идею локальной модели
- Но не будут инвестировать в compliance-специфичные плагины — для них это мелко
- А мы будем наращивать библиотеку плагинов, каждый из которых — best-in-class

Но как сделать обучение быстрым?

Нужен большой объем пользователей, котоыре будут участвовать в обучении. Нужно замотивировать их.

Отсюда появляется идея:  нужно взять маленькую модель. Сделать ее он премис на любом современном устройстве. ЛОкально. Это позволяет нам сохзранить ключевое позиционирование продукта - модель не ходит в интернет и не шарит ваши данные с openai/anthropic.

Но в отличии от general моделей который opensource и в отличии от bedrock который дорог (100баксов в час) и от развертывание своей модели локально - 500к долларов за развертывание малая он премис модель может быть полезна кому угодно. Целевая аудитория на старте: индивидуальные пользователи, которым нужен персональный compliance-советник — люди в инвестициях, малом бизнесе, фрилансеры, консультанты.

Но как сделать, чтобы локальные модели эффективно обучались? Для этого есть федеративное обучение (Federated Learning). Когда локальные обучения после в деперсонализированном виде отсылают обновленные веса в центральный storage НО не отсылают персональные данные.

Proof of concept: Google использует FL с Differential Privacy для ВСЕХ Gboard language models — миллиарды устройств обучают модели без передачи данных в облако (Google Research 2024).

В этом случае мы предположительно можем получить сотни тысяч а может и миллионы компьютеров занимающихся обучением. Но как убедить их это делать?

Нужно сделать модель бесплатной первые пару лет. В замен мы просим обучать модель.

В этом случае мы получаем огромный самообучающийся нетворк из маленьких рабочих станций.

=== PITCH ДЛЯ ИНВЕСТОРОВ: МЫ — ХЕДЖ, А НЕ ЕЩЁ ОДНА СТАВКА ===

Проблема инвестора сейчас:
- 90% AI-стартапов в портфеле = wrapper над OpenAI/Anthropic/Bedrock
- Все зависят от одних и тех же провайдеров
- Риск концентрации: если OpenAI поднимет цены / закроет API / выпустит свой продукт — весь портфель под ударом

Bedrock (AWS) — та же проблема:
- $100/час за inference — дорого для масштабирования
- Данные уходят в облако Amazon
- Vendor lock-in: сменить провайдера = переписать всё
- Если AWS решит конкурировать с вашим портфельным стартапом — он проиграет

Мы — АНТИТЕЗИС текущему портфелю:
- Локально, не в облаке
- Своя модель, не wrapper
- Zero dependency от big tech
- Данные не покидают устройство

Framing: не инвестиция, а СТРАХОВКА

"Вы не инвестируете в нас как в основную ставку.
Вы хеджируете риск того, что через 3 года:
- OpenAI/Anthropic станут монополистами и задерут цены в 10x
- Регуляторы (GDPR, отраслевые) запретят отправку данных в облако
- Локальные модели догонят по качеству (Densing Law это подтверждает)
- Amazon/Google начнут конкурировать с собственными клиентами

Если хоть один из этих сценариев случится — ваш портфель пострадает, а мы взлетим."

Почему $100-200k pre-seed — это правильный размер:
- Для фонда это rounding error (0.5-1% типичного фонда)
- Достаточно для MVP + первых пользователей + proof of federated learning
- Asymmetric bet: если правы → 100x return, если нет → потеряли копейки
- Диверсификация портфеля без существенного риска

Мы не конкурируем с вашими портфельными компаниями — мы страхуем их провал.

=== МОНЕТИЗАЦИЯ ===

После бесплатного периода:
- Подписки для индивидуальных пользователей (персональный compliance-советник)
- Enterprise лицензии для компаний (когда придут легализовывать shadow usage)

В этой точке мы имеем обучающуюся модель. Почему инвесторам может быть допустимо оплатить инвестициями два года работы такой компании? потому что у нас нулевой opex на обслуживание своих серверов. Мы инвестиурем только в интеллекутальные активы команды, которые добиваются идеального соотношения качества и размера модели. Наша цель - ИИ комплаен офицер с доступам к локальным на компьютере данным и онлайн сервисам. Персональный советник, к которому также можно подключить storage с комплаенс документами компании.

Пока все будут тратить деньги на модели, длинные циклы продаж мы с бесплатным продуктом займем большую нишу и построим лучшую модель.

=== GTM: POCKET AI КАК КАНАЛ ПРОНИКНОВЕНИЯ В ENTERPRISE ===

Важно: мы НЕ идём в enterprise напрямую. Никаких длинных циклов продаж, RFP, пилотов с ИТ-отделами.

Факт: ~70% сотрудников уже используют GPT-модели в обход корпоративных политик (Gartner 2024: 69% компаний имеют evidence использования GenAI сотрудниками; Salesforce: >50% используют без formal approval). Им это нужно для работы — они делают это втихую, сливая данные в OpenAI/Anthropic. Это shadow IT в чистом виде.

Наша стратегия: сделать использование ИИ ПЕРСОНАЛЬНЫМ И БЕЗОПАСНЫМ. Локальная модель = "Pocket AI" (Карманный ИИ), который всегда с тобой, но данные не покидают устройство. Сотрудник получает compliance-помощника, компания не теряет данные — даже не зная, что сотрудник этим пользуется.

Путь в enterprise:
1. Идём к end user везде, где ему нужен комплаенс (финансы, юристы, HR, закупки)
2. Сотрудники начинают использовать Pocket AI вместо ChatGPT — втихую, как раньше
3. Через 1-2 года проникновение в компании достигает критической массы
4. ИТ/безопасники обнаруживают нас — но мы УЖЕ безопасны (on-premise, no data leak)
5. Формальное adoption становится легализацией того, что уже используется

Почему вход в enterprise будет лёгким:
- Не нужно убеждать сотрудников — они уже пользователи
- Не нужно доказывать безопасность — on-premise by design
- Не нужен пилот — уже работает в продакшене на их данных
- IT просто формализует то, что уже существует

Это не продажа. Это легализация персонального инструмента.

Единственное требование для этой стратегии: ЛУЧШАЯ МОДЕЛЬ в своём классе. Если модель хуже ChatGPT — сотрудники продолжат сливать данные в OpenAI. Если лучше (для compliance задач) — перейдут к нам.

А потом просто скопируем все фичи конкурентов, котоыре потратили инвестиции на разработку сценариев и их полировку.

Вишенка на торте - после успешного масштабирования и получения лучше и дешевейшей модели на рынке мы сделаем интеграцию в info sec чтобы дать мотивирующую плюшку безопасникам. Это будет что-то мониторинговое, например мониторинг почты или типа того, но с ИИ внутри. ИЛи что-то иное, тут нужно проконсультироваься.

Суть в том, что даже если такой софт продолжит использоваться в персональном режиме (Pocket AI) без центральной закупки, он никогда не отдаст данные в центральное публичное хранилище. Цель: в 20-50 раз дешевле больших моделей (DeepSeek R1 уже в 20-50x дешевле OpenAI; локальный инференс на Apple Silicon снижает costs на 90%+ — данные Inference.net, arxiv 2509.18101). Полностью независим от внешних инфраструктур.

=== ПРОГНОЗЫ И ИССЛЕДОВАНИЯ: ПОЧЕМУ ТРЕНД В НАШУ ПОЛЬЗУ (2025-2030) ===

Источники: Nature Machine Intelligence, MIT Research, Grand View Research, NVIDIA Research, Gartner

--- КЛЮЧЕВОЕ ОТКРЫТИЕ: "DENSING LAW" ---

Nature Machine Intelligence (2025) вводит понятие "capability density" (качество на параметр).
Эмпирический закон: capability density УДВАИВАЕТСЯ каждые 3.5 месяца.

Что это значит практически:
- 2024: для качества GPT-4 нужно ~70B параметров
- 2026: для того же качества нужно ~17B параметров
- 2028: для того же качества нужно ~4B параметров
- 2030: compliance-модель уровня GPT-4 будет работать на любом смартфоне

Драйверы: расширение training data, улучшение качества данных, MoE архитектуры, новые методы compression.

--- РЫНОЧНЫЕ ПРОГНОЗЫ ---

On-Device AI Market:
- 2025: $26.6B
- 2032: $124B (CAGR 24.6%)
Источник: Grand View Research

Small Language Models Market:
- 2023: $7.8B
- 2030: $20.7B (CAGR 15-29%)
Источник: Grand View Research

Edge AI Hardware:
- 2026: $30.7B
- 2031: $68.7B
Источник: GlobeNewswire

MIT Technology Review: SLMs — одна из 10 breakthrough technologies 2025.

--- ТЕХНОЛОГИЧЕСКИЕ ПРОРЫВЫ (ожидаемые) ---

1. HARDWARE (Apple Silicon roadmap):
   - M5 vs M4: 4x speedup в neural accelerator (Apple Research 2025)
   - Каждое поколение: +28% memory bandwidth
   - M4 Max уже сравним с datacenter GPU (546 GB/s)
   - Прогноз M7/M8 (2028-2029): локальный запуск 70B моделей

2. COMPRESSION (Frontiers in Robotics 2025):
   - Hybrid pipelines (pruning + quantization): 75% reduction, 97% accuracy
   - 4-bit quantization — стандарт для on-device
   - DeepSeek-V3: сжат с 1.3TB до 103GB

3. NEUROMORPHIC COMPUTING (долгосрочно 2028-2032):
   - Intel Loihi 2, IBM NorthPole
   - Потенциал: снижение энергопотребления на ПОРЯДКИ
   - Идеально для always-on edge AI

4. SCALING LAWS (MIT-IBM Research):
   - Inference-time scaling важнее pretraining scaling
   - Synthetic data решает проблему нехватки качественных данных
   - Architectural advances важнее pure parameter scaling

--- КЛЮЧЕВЫЕ ПРЕДСКАЗАНИЯ АНАЛИТИКОВ ---

| Источник        | Предсказание                                          | Срок  |
|-----------------|-------------------------------------------------------|-------|
| Gartner         | 33% enterprise apps включают autonomous agents        | 2028  |
| Industry        | 75% enterprise data обрабатывается на edge            | 2025  |
| NVIDIA Research | SLMs — будущее agentic AI (arxiv 2506.02153)          | 2025+ |
| Densing Law     | Capability density x2 каждые 3.5 месяца               | Ongoing |

--- ЧТО ЭТО ЗНАЧИТ ДЛЯ НАШЕЙ СТРАТЕГИИ ---

ПОЗИТИВНЫЕ ТРЕНДЫ:
✓ Модели станут меньше и лучше — 7B модель 2030 = качество 70B модели 2024
✓ Hardware улучшится в 4-8x — M7/M8 запустят модели, которые сегодня требуют серверов
✓ On-device AI станет нормой — $124B рынок, все устройства с локальным AI
✓ Edge computing победит — 75% данных на edge, приватность критична
✓ SLMs для agents — NVIDIA официально: маленькие модели лучше для агентов

РИСКИ:
⚠ Конкуренция усилится — все будут делать то же самое
⚠ Apple может стать конкурентом — они строят экосистему для local AI
⚠ Data moat важнее model moat — качество fine-tuning данных станет ключевым

ВЫВОД:
Наш тезис о маленьких локальных моделях ПОДТВЕРЖДАЕТСЯ рыночными трендами и исследованиями.
Через 5 лет:
- 7B compliance-модель будет работать как GPT-4+ на любом ноутбуке
- On-device AI станет стандартом ($124B рынок)
- Federated learning будет проверенной технологией (Google Gboard proof)
- Fine-tuned SLMs будут доминировать в специализированных задачах

Мы не просто следуем тренду — мы опережаем его, начиная строить инфраструктуру сейчас.


